# -*- coding: utf-8 -*-
"""Titanic Trabalho.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12qo_cAWmpmoboDHCm1kkMdeHlm4wDRjJ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

# Preprocessing
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize

# Machine learning
import catboost
from sklearn.model_selection import train_test_split
from sklearn import model_selection, tree, preprocessing, metrics, linear_model
from sklearn.svm import LinearSVC
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from catboost import CatBoostClassifier, Pool, cv

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

train

train.isnull().sum()

test

test.isnull().sum()

# Salvar PassengerId para auxiliar na criação do arquivo que será enviado ao Kaggle
passengerId = test['PassengerId']

# Criando um DF novo com o train e o test
titanic_df = train.append(test, ignore_index=True)

# Para auxiliar no momento separar o DF titanic_df em train e test
train_index = len(train)
test_index = len(titanic_df) - len(test)

titanic_df

titanic_df.isnull().sum()

titanic_df.info()

train.describe()

# Criando o df onde iremos processar os modelos de ML

df = pd.DataFrame()

titanic_df['Survived'].nunique()

titanic_df['Survived'].unique()

titanic_df['Survived'].isnull().sum()

titanic_df['Survived'].value_counts()

sns.countplot(data = titanic_df, x = 'Survived')

# Criando uma função para ajudar no momento de visualizar as informações de cada coluna

def titanic_func(data, column, count = True):
    print(f'Quantidade de valores únicos: {data[column].nunique()}')
    print(f'\nQuais são os valores únicos: {data[column].unique()}')
    print(f'\nQuantidade de valores nulos: {data[column].isnull().sum()}')
    print(f'\nQuantidade por opção: \n{data[column].value_counts()}')

    if count == True:
        sns.countplot(data = data, x = column, hue = 'Survived')
    else:
        sns.displot(data[column], kde = True)


titanic_func(titanic_df, 'Survived')

df['Survived'] = titanic_df['Survived']

df

titanic_func(titanic_df, 'Pclass')

df['Pclass'] = titanic_df['Pclass']
df.head()

"""Sexo"""

titanic_df['Sex'].unique()

titanic_df['Sex'] = titanic_df['Sex'].replace(['female', 'male'], [1, 0])

titanic_func(titanic_df, 'Sex')

df['Sex'] = titanic_df['Sex']
df.head()

"""Idade"""

titanic_func(titanic_df, 'Age', False)

titanic_df.corr()

titanic_df[titanic_df['Pclass'] == 1]['Age'].mean()

titanic_df[titanic_df['Pclass'] == 2]['Age'].mean()

titanic_df[titanic_df['Pclass'] == 3]['Age'].mean()

for i in sorted(titanic_df['Pclass'].unique()):
    print(f"Pessoas da {i}ª classe tem a média de idade de: {titanic_df[titanic_df['Pclass'] == i]['Age'].mean():.0f} anos.")

titanic_df[titanic_df['Pclass'] == 1]['Age'].isnull().sum()

titanic_df['Age'].isnull().sum()

df_age_mean_pclass_1 = titanic_df[titanic_df['Pclass']==1].groupby('Pclass')['Age'].mean()
df_age_mean_pclass_2 = titanic_df[titanic_df['Pclass']==2].groupby('Pclass')['Age'].mean()
df_age_mean_pclass_3 = titanic_df[titanic_df['Pclass']==3].groupby('Pclass')['Age'].mean()

df['Age'] = titanic_df['Age'].fillna(titanic_df['Pclass'].map({1: df_age_mean_pclass_1.loc[1], 2: df_age_mean_pclass_2.loc[2], 3: df_age_mean_pclass_3.loc[3]}))

titanic_df[titanic_df['Pclass'] == 1]['Age'].isnull().sum()

titanic_df['Age'].isnull().sum()

titanic_df['Age']

titanic_df.info()

df['Age'] = titanic_df['Age']
df.head()

titanic_func(titanic_df, 'SibSp')

df['SibSp'] = titanic_df['SibSp']
df.head()

"""Parch"""

titanic_func(titanic_df, 'Parch')

df['Parch'] = titanic_df['Parch']
df.head()

"""Tamanho da Familia"""

titanic_df['FamilySize'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1

df['FamilySize'] = titanic_df['FamilySize']
df.head()

"""Fare"""

titanic_func(titanic_df, 'Fare', False)

titanic_df[titanic_df['Fare'].isnull()]

titanic_df[titanic_df['Pclass'] == 3]['Fare'].mean()

titanic_df['Fare'].fillna(titanic_df[titanic_df['Pclass'] == 3]['Fare'].mean(), inplace = True)

titanic_df.isnull().sum()

df['Fare'] = titanic_df['Fare']
df.head()

"""Cabin"""

titanic_df.head(2)

titanic_df['Cabin'].isnull().sum()

titanic_df['Cabin'].unique()

"""Embarked"""

titanic_func(titanic_df, 'Embarked')

titanic_df[titanic_df['Embarked'] == "S"]['Survived'].mean()

titanic_df[titanic_df['Embarked'] == "S"]['Pclass'].mean()

titanic_df[titanic_df['Embarked'] == "C"]['Survived'].mean()

titanic_df[titanic_df['Embarked'] == "C"]['Pclass'].mean()

titanic_df[titanic_df['Embarked'] == "Q"]['Survived'].mean()

titanic_df[titanic_df['Embarked'] == "Q"]['Pclass'].mean()

titanic_df[titanic_df['Embarked'].isnull()]

titanic_df['Embarked'].fillna('C', inplace = True)

titanic_df.isnull().sum()

df['Embarked'] = titanic_df['Embarked']
df.head()

"""Name"""

titanic_df.head()

titanic_df['Name']

titanic_df['Title'] = titanic_df['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())

titanic_df['Title'].nunique()

titanic_df['Title'].unique()

titanic_df['Title'].value_counts()

titanic_df['Title'] = [n if n in ['Mr', 'Miss', 'Mrs', 'Master'] else 'Person' for n in titanic_df['Title']]

titanic_df

df['Title'] = titanic_df['Title']
df.head()

titanic_func(titanic_df, 'Title')

titanic_df.isnull().sum()

df

pclass = pd.get_dummies(df['Pclass'], prefix = "Pclass", drop_first = True)
title = pd.get_dummies(df['Title'], prefix = 'Title', drop_first = True)
embarked = pd.get_dummies(df['Embarked'], prefix = 'Embarked', drop_first = True)

titanic_completo = pd.concat([df, pclass, title, embarked], axis = 1)

titanic_completo.drop(['Pclass', 'Title', 'Embarked'], axis=1, inplace=True)

titanic_completo

train = titanic_completo[:train_index].copy()
test = titanic_completo[test_index:].copy()

train.info()

train['Survived'] = train['Survived'].astype(int)

X = train.drop('Survived', axis = 1)
y = train['Survived']

X_test = test.drop('Survived', axis = 1)

# Função que processa o algoritmo e devolve a acurácia

def func_acuracia(algoritmo, X_train, y_train, vc):
    modelo = algoritmo.fit(X_train, y_train)
    acuracia = round(modelo.score(X_train, y_train) * 100, 2)

    train_pred = model_selection.cross_val_predict(algoritmo, X_train, y_train, cv = vc, n_jobs = -1)
    acuracia_vc = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)

    return acuracia, acuracia_vc

"""Aprendizado de maquina - metodos
---


"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X)
rf = RandomForestClassifier()
rf.fit(X_filled, y)
acc_rf = rf.score(X_filled, y)

acc_vc_rf = func_acuracia(rf, X_filled, y, 10)

print(f"Acurácia: {acc_rf}")
print(f"Acurácia Validação Cruzada: {acc_vc_rf}")

from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X)
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_filled, y)
acc_log = logreg.score(X_filled, y)

acc_vc_log = func_acuracia(logreg, X_filled, y, 10)

print(f"Acurácia: {acc_log}")
print(f"Acurácia Validação Cruzada: {acc_vc_log}")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X)
knn = KNeighborsClassifier()
knn.fit(X_filled, y)
acc_knn = knn.score(X_filled, y)

acc_vc_knn = func_acuracia(knn, X_filled, y, 10)

print(f"Acurácia: {acc_knn}")
print(f"Acurácia Validação Cruzada: {acc_vc_knn}")

from sklearn.naive_bayes import GaussianNB
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X)
gnb = GaussianNB()
gnb.fit(X_filled, y)
acc_gaussian = gnb.score(X_filled, y)

acc_vc_gaussian = func_acuracia(gnb, X_filled, y, 10)

print(f"Acurácia: {acc_gaussian}")
print(f"Acurácia Validação Cruzada: {acc_vc_gaussian}")

from sklearn.svm import LinearSVC
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X)
svc = LinearSVC(dual=False)
svc.fit(X_filled, y)
acc_linear_svc = svc.score(X_filled, y)

acc_vc_linear_svc = func_acuracia(svc, X_filled, y, 10)

print(f"Acurácia: {acc_linear_svc}")
print(f"Acurácia Validação Cruzada: {acc_vc_linear_svc}")

from sklearn.linear_model import SGDClassifier
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X)
sgd = SGDClassifier()
sgd.fit(X_filled, y)
acc_sgd = sgd.score(X_filled, y)

acc_vc_sgd = func_acuracia(sgd, X_filled, y, 10)

print(f"Acurácia: {acc_sgd}")
print(f"Acurácia Validação Cruzada: {acc_vc_sgd}")

from sklearn.tree import DecisionTreeClassifier
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X)
dt = DecisionTreeClassifier()
dt.fit(X_filled, y)
acc_dt = dt.score(X_filled, y)

acc_vc_dt = func_acuracia(dt, X_filled, y, 10)

print(f"Acurácia: {acc_dt}")
print(f"Acurácia Validação Cruzada: {acc_vc_dt}")

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X)
gbt = GradientBoostingClassifier()
gbt.fit(X_filled, y)
acc_gbt = gbt.score(X_filled, y)

acc_vc_gbt = func_acuracia(gbt, X_filled, y, 10)

print(f"Acurácia: {acc_gbt}")
print(f"Acurácia Validação Cruzada: {acc_vc_gbt}")

params = dict(
    max_depth = [n for n in range(1, 5)],
    min_samples_split = [n for n in range(2, 6)],
    min_samples_leaf = [n for n in range(2, 6)],
    n_estimators = [n for n in range(10, 50, 10)],
)

gbc = GradientBoostingClassifier()

from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
params = {
    'max_depth': [1, 2, 3, 4],
    'min_samples_leaf': [2, 3, 4, 5],
    'learning_rate': [0.1, 0.01, 0.001],
    'max_iter': [100, 200, 300],
}
hgb = HistGradientBoostingClassifier()
hgb_cv = GridSearchCV(estimator=hgb, param_grid=params, cv=10)
hgb_cv = GridSearchCV(estimator=hgb, param_grid=params, cv=10)
hgb_cv.fit(X, y)
best_params = hgb_cv.best_params_
best_acc = hgb_cv.best_score_

print(f"Melhores parâmetros: {best_params}")
print(f"Melhor acurácia: {best_acc}")

gbc_cv = GridSearchCV(estimator = gbc, param_grid = params, cv = 10)
gbc_cv.fit(X, y)

print(f"Melhor pontuação: {gbc_cv.best_score_}")
print(f"Melhores parâmetros: {gbc_cv.best_estimator_}")

gradientBoostingClassifier_pred = gbc_cv.predict(X_test)

kaggle = pd.DataFrame({'PassengerId': passengerId, 'Survived': gradientBoostingClassifier_pred})
kaggle.to_csv('./titanic_gradient_boosting_pred.csv', index=False)